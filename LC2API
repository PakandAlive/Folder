addEventListener('fetch', event => {
  event.respondWith(handleRequest(event.request))
})

async function handleRequest(request) {
  const url = new URL(request.url)
  // 处理CORS预检请求
  if (request.method === 'OPTIONS') {
    return handleCORS()
  }
  // 路由处理
  if (url.pathname === '/v1/models' && request.method === 'GET') {
    return handleModels()
  } else if (url.pathname === '/v1/chat/completions' && request.method === 'POST') {
    return handleChatCompletions(request)
  } else if (url.pathname === '/health' && request.method === 'GET') {
    return handleHealth()
  } else if (url.pathname === '/debug' && request.method === 'GET') {
    return handleDebug(request)
  } else {
    return new Response('Not Found', { status: 404 })
  }
}

// 处理CORS
function handleCORS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
      'Access-Control-Allow-Headers': 'Origin, X-Requested-With, Content-Type, Accept, Authorization',
    }
  })
}

// 设置CORS头
function setCORSHeaders(response) {
  response.headers.set('Access-Control-Allow-Origin', '*')
  response.headers.set('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS')
  response.headers.set('Access-Control-Allow-Headers', 'Origin, X-Requested-With, Content-Type, Accept, Authorization')
  return response
}

// 将消息历史转换为上下文字符串
function formatMessagesAsContext(messages) {
  if (!messages || messages.length === 0) {
    return ''
  }

  // 将所有消息组合成一个连续的对话上下文
  let context = ''
  for (const message of messages) {
    if (message.role === 'system') {
      context += `系统: ${message.content}\n\n`
    } else if (message.role === 'user') {
      context += `用户: ${message.content}\n\n`
    } else if (message.role === 'assistant') {
      context += `助手: ${message.content}\n\n`
    }
  }

  // 获取最后一条用户消息作为当前问题
  const lastUserMessage = messages.filter(msg => msg.role === 'user').pop()
  if (lastUserMessage) {
    context += `请回答最后一个用户问题: ${lastUserMessage.content}`
  }

  return context.trim()
}

// 健康检查端点
async function handleHealth() {
  const healthResponse = {
    status: "healthy",
    timestamp: new Date().toISOString(),
    version: "1.0.0",
    uptime: Date.now()
  }
  const response = new Response(JSON.stringify(healthResponse), {
    headers: { 'Content-Type': 'application/json' }
  })
  return setCORSHeaders(response)
}

// 调试信息端点
async function handleDebug(request) {
  const debugInfo = {
    timestamp: new Date().toISOString(),
    headers: Object.fromEntries(request.headers.entries()),
    method: request.method,
    url: request.url,
    cf: request.cf || null
  }
  const response = new Response(JSON.stringify(debugInfo, null, 2), {
    headers: { 'Content-Type': 'application/json' }
  })
  return setCORSHeaders(response)
}

// 处理 /v1/models 端点
async function handleModels() {
  const modelsResponse = {
    "object": "list",
    "data": [
      {
        "id": "longcat-flash",
        "object": "model",
        "created": Math.floor(Date.now() / 1000),
        "owned_by": "longcat",
        "permission": [],
        "root": "longcat-flash",
        "parent": null
      },
      {
        "id": "longcat-thinking",
        "object": "model",
        "created": Math.floor(Date.now() / 1000),
        "owned_by": "longcat",
        "permission": [],
        "root": "longcat-thinking",
        "parent": null
      }
    ]
  }
  const response = new Response(JSON.stringify(modelsResponse), {
    headers: { 'Content-Type': 'application/json' }
  })
  return setCORSHeaders(response)
}

// 重试配置
const MAX_RETRIES = 3
const RETRY_DELAY = 1000 // 1秒

// 延迟函数
function delay(ms) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

// 带重试的fetch函数
async function fetchWithRetry(url, options, retries = MAX_RETRIES) {
  for (let i = 0; i <= retries; i++) {
    try {
      console.log(`尝试请求 ${url}, 第 ${i + 1} 次`)
      const response = await fetch(url, options)
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }
      
      console.log(`请求成功: ${response.status}`)
      return response
    } catch (error) {
      console.error(`请求失败 (第 ${i + 1} 次):`, error.message)
      
      if (i === retries) {
        throw new Error(`请求失败，已重试 ${retries + 1} 次: ${error.message}`)
      }
      
      await delay(RETRY_DELAY * (i + 1)) // 递增延迟
    }
  }
}

// 处理 /v1/chat/completions 端点
async function handleChatCompletions(request) {
  try {
    const body = await request.json()
    const { model, messages, stream, temperature, ...otherParams } = body

    console.log('接收到聊天请求:', { model, messageCount: messages?.length, stream })

    const isThinking = model === 'longcat-thinking'

    // 将消息历史转换为包含上下文的内容
    const contextualContent = formatMessagesAsContext(messages)

    // 构建long.chat格式的请求体
    const longChatPayload = {
      stream: stream || false,
      temperature: temperature || 0.7,
      content: contextualContent,
      messages: messages // 仍然传递原始消息，以防API支持
    }

    if (isThinking) {
      longChatPayload.reasonEnabled = 1
      longChatPayload.searchEnabled = 0
      longChatPayload.regenerate = 0
    }

    // 设置请求头
    const headers = {
      'accept': stream ? 'text/event-stream,application/json' : 'application/json',
      'content-type': 'application/json',
      'origin': 'https://longcat.chat',
      'referer': 'https://longcat.chat/t',
      'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    console.log('准备发送请求到 longcat.chat')

    // 向long.chat发送请求（带重试）
    const longChatResponse = await fetchWithRetry('https://longcat.chat/api/v1/chat-completion-oversea', {
      method: 'POST',
      headers: headers,
      body: JSON.stringify(longChatPayload)
    })

    if (stream) {
      // 流式响应处理
      const { readable, writable } = new TransformStream()
      // 异步处理流数据转换
      processStreamResponse(longChatResponse, writable, isThinking, model)
      const response = new Response(readable, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive'
        }
      })
      return setCORSHeaders(response)
    } else {
      // 非流式响应 - 读取完整的流数据并提取最终内容
      const reader = longChatResponse.body.getReader()
      const decoder = new TextDecoder()
      let fullContent = ''
      let tokenInfo = null
    
      let thinkContent = ''
      let contentContent = ''
    
      try {
        let buffer = ''
        while (true) {
          const { done, value } = await reader.read()
          if (done) break
        
          buffer += decoder.decode(value, { stream: true })
          const lines = buffer.split('\n')
          buffer = lines.pop() || ''
        
          for (const line of lines) {
            if (line.trim() && line.startsWith('data:')) {
              try {
                const dataStr = line.slice(5).trim()
                if (dataStr === '[DONE]') continue
              
                const longcatData = JSON.parse(dataStr)
              
                if (longcatData.event?.type === 'think') {
                  if (isThinking) {
                    thinkContent = longcatData.event.content || ''
                  }
                } else if (longcatData.event?.type === 'content') {
                  contentContent = longcatData.event.content || ''
                } else if (longcatData.event?.type === 'finish' && longcatData.event.usage) {
                  tokenInfo = {
                    promptTokens: longcatData.event.usage.inputTokens || 0,
                    completionTokens: longcatData.event.usage.outputTokens || 0,
                    totalTokens: longcatData.event.usage.totalTokens || 0
                  }
                } else if (longcatData.tokenInfo) {
                  tokenInfo = longcatData.tokenInfo
                }
              } catch (e) {
                // 忽略解析错误
                continue
              }
            }
          }
        }
      
        // 处理缓冲区中剩余的数据
        if (buffer.trim() && buffer.startsWith('data:')) {
          try {
            const dataStr = buffer.slice(5).trim()
            if (dataStr !== '[DONE]') {
              const longcatData = JSON.parse(dataStr)
              if (longcatData.event?.type === 'think') {
                if (isThinking) {
                  thinkContent = longcatData.event.content || ''
                }
              } else if (longcatData.event?.type === 'content') {
                contentContent = longcatData.event.content || ''
              } else if (longcatData.event?.type === 'finish' && longcatData.event.usage) {
                tokenInfo = {
                  promptTokens: longcatData.event.usage.inputTokens || 0,
                  completionTokens: longcatData.event.usage.outputTokens || 0,
                  totalTokens: longcatData.event.usage.totalTokens || 0
                }
              } else if (longcatData.tokenInfo) {
                tokenInfo = longcatData.tokenInfo
              }
            }
          } catch (e) {
            // 忽略解析错误
          }
        }
      
      } catch (error) {
        console.error('读取非流式响应错误:', {
          message: error.message,
          stack: error.stack,
          timestamp: new Date().toISOString()
        })
        throw error
      }

      if (isThinking) {
        fullContent = `<think>${thinkContent}</think>${contentContent}`
      } else {
        fullContent = contentContent
      }
    
      // 转换为OpenAI格式 - 只包含标准字段
      const openaiResponse = {
        id: `chatcmpl-${Date.now()}`,
        object: "chat.completion",
        created: Math.floor(Date.now() / 1000),
        model: model,
        choices: [{
          index: 0,
          message: {
            role: "assistant",
            content: fullContent
          },
          finish_reason: "stop"
        }],
        usage: {
          prompt_tokens: tokenInfo?.promptTokens || 0,
          completion_tokens: tokenInfo?.completionTokens || 0,
          total_tokens: tokenInfo?.totalTokens || 0
        }
      }
    
      const response = new Response(JSON.stringify(openaiResponse), {
        headers: { 'Content-Type': 'application/json' }
      })
      return setCORSHeaders(response)
    }
  } catch (error) {
    console.error('API转发错误详情:', {
      message: error.message,
      stack: error.stack,
      timestamp: new Date().toISOString()
    })
    
    // 根据错误类型返回不同的状态码和信息
    let statusCode = 500
    let errorType = "api_error"
    let errorMessage = error.message
    
    if (error.message.includes('请求失败，已重试')) {
      statusCode = 502
      errorType = "upstream_error"
      errorMessage = "上游服务暂时不可用，请稍后重试"
    } else if (error.message.includes('HTTP 4')) {
      statusCode = 400
      errorType = "bad_request"
    } else if (error.message.includes('JSON')) {
      statusCode = 400
      errorType = "invalid_json"
      errorMessage = "请求格式错误"
    }
    
    const errorResponse = new Response(JSON.stringify({
      error: {
        message: errorMessage,
        type: errorType,
        details: error.message // 保留原始错误信息用于调试
      }
    }), {
      status: statusCode,
      headers: { 'Content-Type': 'application/json' }
    })
    return setCORSHeaders(errorResponse)
  }
}

// 处理流式响应转换
async function processStreamResponse(longChatResponse, writable, isThinking, model) {
  const writer = writable.getWriter()
  const reader = longChatResponse.body.getReader()
  const decoder = new TextDecoder()
  const encoder = new TextEncoder()

  try {
    let buffer = ''
    let currentThink = ''
    let currentContent = ''
    let thinkStarted = false
    let thinkFinished = false

    while (true) {
      const { done, value } = await reader.read()
      if (done) break
    
      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
    
      // 保留最后一行（可能不完整）
      buffer = lines.pop() || ''
    
      for (const line of lines) {
        if (line.trim() && line.startsWith('data:')) {
          try {
            const dataStr = line.slice(5).trim()
          
            // 处理 [DONE] 信号
            if (dataStr === '[DONE]') {
              await writer.write(encoder.encode('data: [DONE]\n\n'))
              continue
            }
          
            // 解析 longcat.chat 的数据格式
            const longcatData = JSON.parse(dataStr)
          
            if (longcatData.event?.type === 'think' && isThinking) {
              const newThink = longcatData.event.content || ''
              if (!thinkStarted) {
                // 发送 <think> 开头
                const openaiChunk = createOpenAIChunk(model, '<think>')
                await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
                thinkStarted = true
              }
              // 计算 delta
              const delta = newThink.slice(currentThink.length)
              if (delta) {
                const openaiChunk = createOpenAIChunk(model, delta)
                await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
              }
              currentThink = newThink
              if (longcatData.event.status === 'FINISHED') {
                // 发送 </think> 结尾
                const openaiChunk = createOpenAIChunk(model, '</think>')
                await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
                thinkFinished = true
              }
            } else if (longcatData.event?.type === 'content') {
              const newContent = longcatData.event.content || ''
              const delta = newContent.slice(currentContent.length)
              if (delta) {
                const openaiChunk = createOpenAIChunk(model, delta)
                await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
              }
              currentContent = newContent
              if (longcatData.event.status === 'FINISHED') {
                const openaiChunk = createOpenAIChunk(model, '', 'stop')
                await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
              }
            }
            // 如果是最后一个，发送 [DONE]
            if (longcatData.lastOne) {
              await writer.write(encoder.encode('data: [DONE]\n\n'))
            }
          } catch (e) {
            console.error('解析流数据错误:', e)
            continue
          }
        }
      }
    }
  
    // 处理缓冲区中剩余的数据
    if (buffer.trim() && buffer.startsWith('data:')) {
      try {
        const dataStr = buffer.slice(5).trim()
        if (dataStr !== '[DONE]') {
          const longcatData = JSON.parse(dataStr)
          if (longcatData.event?.type === 'think' && isThinking) {
            const newThink = longcatData.event.content || ''
            if (!thinkStarted) {
              const openaiChunk = createOpenAIChunk(model, '<think>')
              await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
              thinkStarted = true
            }
            const delta = newThink.slice(currentThink.length)
            if (delta) {
              const openaiChunk = createOpenAIChunk(model, delta)
              await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
            }
            currentThink = newThink
            if (longcatData.event.status === 'FINISHED') {
              const openaiChunk = createOpenAIChunk(model, '</think>')
              await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
              thinkFinished = true
            }
          } else if (longcatData.event?.type === 'content') {
            const newContent = longcatData.event.content || ''
            const delta = newContent.slice(currentContent.length)
            if (delta) {
              const openaiChunk = createOpenAIChunk(model, delta)
              await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
            }
            currentContent = newContent
            if (longcatData.event.status === 'FINISHED') {
              const openaiChunk = createOpenAIChunk(model, '', 'stop')
              await writer.write(encoder.encode(`data: ${JSON.stringify(openaiChunk)}\n\n`))
            }
          }
        }
      } catch (e) {
        console.error('处理缓冲区数据错误:', e)
      } 
    }
  
  } catch (error) {
    console.error('流处理错误:', error)
  } finally {
    await writer.close()
  }
}

// Helper function to create OpenAI chunk
function createOpenAIChunk(model, content, finishReason = null) {
  return {
    id: `chatcmpl-${Date.now()}`,
    object: "chat.completion.chunk",
    created: Math.floor(Date.now() / 1000),
    model: model,
    choices: [{
      index: 0,
      delta: {
        content: content
      },
      finish_reason: finishReason
    }]
  }
}
